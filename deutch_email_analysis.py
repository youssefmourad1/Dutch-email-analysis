# -*- coding: utf-8 -*-
"""deutch email analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IDWZBKpbK_JkSmc4mfwsFn8CNJSc6SfE
"""

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import itertools
import matplotlib.pyplot as plt

# Load the dataset
file_path_xls = 'Data sample (1).xls'
data_csv = pd.read_excel(file_path_xls)

import re

def extract_questions(text):
    # Check if the input is a string
    if not isinstance(text, str):
        # Return an empty list or other default value if the input is not a string
        return []

    # Regular expression to find sentences ending with a question mark
    question_pattern = r'[^.?!\n]*\?'
    questions = re.findall(question_pattern, text)
    return questions

data_csv['Questions'] = data_csv['MsgBodyPlainText'].apply(extract_questions)

# # Display the results
# data_csv[['MsgBodyPlainText', 'Questions']].head()



# Flatten the list of questions to a single list
all_questions = list(itertools.chain(*data_csv['Questions']))

# Preprocess questions (a more thorough preprocessing might be needed depending on the data)
all_questions_preprocessed = [q.lower() for q in all_questions if q]  # Basic preprocessing for demonstration

# Vectorize the questions
vectorizer = TfidfVectorizer()
question_vectors = vectorizer.fit_transform(all_questions_preprocessed)

# Compute cosine similarity matrix
cosine_sim = cosine_similarity(question_vectors)

# Initialize clusters (using the index of questions in the all_questions_preprocessed list)
clusters = []

# Threshold for considering questions to be similar
similarity_threshold = 0.7

# Iterate over the cosine similarity matrix
for i in range(cosine_sim.shape[0]):
    found_cluster = False
    for cluster in clusters:
        if any(cosine_sim[i, j] > similarity_threshold for j in cluster):
            cluster.append(i)
            found_cluster = True
            break
    if not found_cluster:
        clusters.append([i])

# Select a representative for each cluster (for simplicity, we'll choose the first question in each cluster)
representative_questions = [all_questions_preprocessed[cluster[0]] for cluster in clusters]

# Count of each representative question
question_counts = {question: len(cluster) for question, cluster in zip(representative_questions, clusters)}

"""Optoional for visualziation and logging of all the questions"""
# Display the counts of each representative question found
# for question, count in question_counts.items():
#     print(f"Question: \"{question}\" Count: {count}")

# Count the number of questions in each cluster and filter out those with less than 10 characters
question_counts = {question: len(cluster) for question, cluster in zip(representative_questions, clusters) if len(question) >= 10}

# Sort question_counts to a DataFrame for easier handling
questions_df = pd.DataFrame(list(question_counts.items()), columns=['Question', 'Count']).sort_values(by='Count', ascending=False)

# Select the top n questions
n = 5  # Adjust n based on your requirement
top_questions_df = questions_df.head(n)

# Create a horizontal bar plot for the top n questions
plt.figure(figsize=(10, 8))
plt.barh(top_questions_df['Question'], top_questions_df['Count'], color='steelblue')
plt.xlabel('Frequency')
plt.ylabel('Questions')
plt.title('Top N Most Frequent Questions (with 10+ characters)')
plt.gca().invert_yaxis()  # Display the highest frequency question on top
plt.show()

